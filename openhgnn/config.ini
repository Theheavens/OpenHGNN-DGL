
[NSHE]
learning_rate = 0.008
beta = 0.05
seed = 0
norm_emd_flag = True

project_dim=128
context_dim=64
emd_dim=128

num_e_neg = 1
num_ns_neg = 4
max_epoch = 500

optimizer = adam

[GTN]
learning_rate = 0.005
weight_decay = 0.001
max_epoch = 40
emd_dim = 64
num_channels = 2
num_layers = 3
seed = 0

norm_emd_flag = True
adaptive_lr_flag = False
sparse = True

[RSHN]
learning_rate = 0.01
weight_decay = 5e-4
dropout = 0.6

seed = 0
dim = 16
max_epoch = 500
rw_len = 4
batch_size = 5000
num_node_layer = 2
num_edge_layer = 2
patience = 50
validation = False
minibatch_flag = False

[RGCN]
learning_rate = 0.01
weight_decay = 0
dropout = 0

seed =0
n_hidden = 16
n_bases = -1
n_layers = 2

max_epoch = 100
patience = 50
batch_size = 126
fanout = 4

validation = True
use_self_loop = False
minibatch_flag = True

[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.5

seed = 0
n_layers = 1
n_hidden = 100
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
patience = 100

comp_fn = sub
validation = True
minibatch_flag = False

[HetGNN]
learning_rate = 0.01
weight_decay = 0.0001
dim = 128
max_epoch = 100
batch_size = 32
window_size = 5
num_workers =0
batches_per_epoch = 20

rw_length = 10
rw_walks = 30
rwr_prob = 0.5

seed = 0
patience = 100

minibatch_flag = True
